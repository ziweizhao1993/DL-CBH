{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import modules and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "print (torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load features\n",
    "features = np.load(\"/l/ear/electrode/users/zz47/wiki_crop/features.npy\")\n",
    "target_classes = np.load(\"/l/ear/electrode/users/zz47/wiki_crop/target_classes.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use base_model to extract feature from image with batchsize=100\n",
    "mean = torch.tensor([129.186279296875, 104.76238250732422, 93.59396362304688]).view(1, -1, 1, 1)\n",
    "def extract_features(images, model):\n",
    "    model.eval()\n",
    "    N = np.shape(images)[0]\n",
    "    if len(images.shape) == 4:\n",
    "        new_features = np.zeros([N,2622])\n",
    "    else:\n",
    "        new_features = np.zeros([N,64])\n",
    "    for i in range(0,N,100):\n",
    "        if (N-i)>=100:\n",
    "            batch_size = 100\n",
    "        else:\n",
    "            batch_size = N-i\n",
    "        if len(images.shape) == 4:\n",
    "            temp_feature = torch.Tensor(np.transpose(images[i:i+batch_size,:,:,:], [0,3,1,2]))\n",
    "            temp_feature.sub_(mean)\n",
    "            temp_feature /= 255.0\n",
    "            temp_feature = temp_feature.to('cuda')\n",
    "        else:\n",
    "            temp_feature = torch.Tensor(images[i:i+batch_size,:]).to('cuda')\n",
    "        \n",
    "        with torch.no_grad():   \n",
    "            temp_feature = model(temp_feature)\n",
    "        new_features[i:i+batch_size,:] = temp_feature.to('cpu').detach().numpy()\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nearest Neighbor\n",
    "def l1_1NN(matrix, vector):\n",
    "    diff_matrix = matrix - vector\n",
    "    diff_vector = np.sum(np.abs(diff_matrix),axis=1)\n",
    "    minidx = np.argmin(diff_vector)\n",
    "    return minidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(target_classes.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#novel setting\n",
    "from sklearn.model_selection import KFold\n",
    "test_mask1 = target_classes < 20\n",
    "X_test1 = features[test_mask1,:,:,:]\n",
    "y_test1 = target_classes[test_mask1]\n",
    "test_mask2 = (target_classes >= 50) & (target_classes < 70)\n",
    "X_test2 = features[test_mask2,:,:,:]\n",
    "y_test2 = target_classes[test_mask2]\n",
    "test_mask3 = target_classes >= 70\n",
    "X_test3 = features[test_mask3,:,:,:]\n",
    "y_test3 = target_classes[test_mask3]\n",
    "\n",
    "train_mask = ((target_classes < 50) & (target_classes >= 20))\n",
    "X_train = features[train_mask,:,:,:]\n",
    "y_train = target_classes[train_mask]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "kf.get_n_splits(X_train)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print (i)\n",
    "    #fold 8\n",
    "    if i == 8:\n",
    "        X_train, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_train, y_val = y_train[train_index], y_train[val_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of samples\n",
    "print (X_train.shape)\n",
    "print (X_val.shape)\n",
    "print (X_test1.shape)\n",
    "print (X_test2.shape)\n",
    "print (X_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_y_train = scaler.transform(y_train.reshape(-1, 1))[:,0]\n",
    "scaled_y_val = scaler.transform(y_val.reshape(-1, 1))[:,0]\n",
    "scaled_y_test1 = scaler.transform(y_test1.reshape(-1, 1))[:,0]\n",
    "scaled_y_test2 = scaler.transform(y_test2.reshape(-1, 1))[:,0]\n",
    "scaled_y_test3 = scaler.transform(y_test3.reshape(-1, 1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(y, scaler):\n",
    "    #same as scaler.inverse_transform\n",
    "    return y * np.sqrt(scaler.var_) + scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load base_model\n",
    "class Vgg_face(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Vgg_face, self).__init__()\n",
    "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x1 = self.conv1_1(x0)\n",
    "        x2 = self.relu1_1(x1)\n",
    "        x3 = self.conv1_2(x2)\n",
    "        x4 = self.relu1_2(x3)\n",
    "        x5 = self.pool1(x4)\n",
    "        x6 = self.conv2_1(x5)\n",
    "        x7 = self.relu2_1(x6)\n",
    "        x8 = self.conv2_2(x7)\n",
    "        x9 = self.relu2_2(x8)\n",
    "        x10 = self.pool2(x9)\n",
    "        x11 = self.conv3_1(x10)\n",
    "        x12 = self.relu3_1(x11)\n",
    "        x13 = self.conv3_2(x12)\n",
    "        x14 = self.relu3_2(x13)\n",
    "        x15 = self.conv3_3(x14)\n",
    "        x16 = self.relu3_3(x15)\n",
    "        x17 = self.pool3(x16)\n",
    "        x18 = self.conv4_1(x17)\n",
    "        x19 = self.relu4_1(x18)\n",
    "        x20 = self.conv4_2(x19)\n",
    "        x21 = self.relu4_2(x20)\n",
    "        x22 = self.conv4_3(x21)\n",
    "        x23 = self.relu4_3(x22)\n",
    "        x24 = self.pool4(x23)\n",
    "        x25 = self.conv5_1(x24)\n",
    "        x26 = self.relu5_1(x25)\n",
    "        x27 = self.conv5_2(x26)\n",
    "        x28 = self.relu5_2(x27)\n",
    "        x29 = self.conv5_3(x28)\n",
    "        x30 = self.relu5_3(x29)\n",
    "        x31_preflatten = self.pool5(x30)\n",
    "        x31 = x31_preflatten.contiguous().view(x31_preflatten.size(0), -1)\n",
    "        x32 = self.fc6(x31)\n",
    "        x33 = self.relu6(x32)\n",
    "        x34 = self.dropout6(x33)\n",
    "        x35 = self.fc7(x34)\n",
    "        x36 = self.relu7(x35)\n",
    "        x37 = self.dropout7(x36)\n",
    "        x38 = self.fc8(x37)\n",
    "        return x38\n",
    "\n",
    "def vgg_face(weights_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    load imported model instance\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): If set, loads model weights from the given path\n",
    "    \"\"\"\n",
    "    model = Vgg_face()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "#base_model = nn.DataParallel(vgg_face(\"/l/ear/electrode/users/zz47/vgg_face_dag.pth\")).to('cuda')\n",
    "base_model = vgg_face(\"/l/ear/electrode/users/zz47/vgg_face_dag.pth\").to('cuda')\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use base_model to extract feature from image with batchsize=100\n",
    "X_train_features = extract_features(X_train, base_model)\n",
    "X_val_features = extract_features(X_val, base_model)\n",
    "X_test1_features = extract_features(X_test1, base_model)\n",
    "X_test2_features = extract_features(X_test2, base_model)\n",
    "X_test3_features = extract_features(X_test3, base_model)\n",
    "\n",
    "print (np.shape(X_train_features))\n",
    "print (np.shape(X_val_features))\n",
    "print (np.shape(X_test1_features))\n",
    "print (np.shape(X_test2_features))\n",
    "print (np.shape(X_test3_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random guess\n",
    "#use average of training set to guess test set\n",
    "print (np.mean(np.abs(y_test1-np.mean(y_train))))\n",
    "print (np.mean(np.abs(y_test2-np.mean(y_train))))\n",
    "print (np.mean(np.abs(y_test3-np.mean(y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular dataset\n",
    "class FeatureDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.x)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx,:].astype('float32')\n",
    "        target_class = self.y[idx].astype('float32')\n",
    "\n",
    "        return feature, target_class\n",
    "\n",
    "train_dataset = FeatureDataset(X_train_features, scaled_y_train)\n",
    "val_dataset = FeatureDataset(X_val_features, scaled_y_val)\n",
    "test1_dataset = FeatureDataset(X_test1_features, scaled_y_test1)\n",
    "test2_dataset = FeatureDataset(X_test2_features, scaled_y_test2)\n",
    "test3_dataset = FeatureDataset(X_test3_features, scaled_y_test3)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "test1_dataloader = DataLoader(test1_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "test2_dataloader = DataLoader(test2_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "test3_dataloader = DataLoader(test3_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model\n",
    "class Base_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2622, 512)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(512, 16)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "base_regression = Base_regression().to('cuda')\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(base_regression.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    criterion = nn.L1Loss()\n",
    "    total_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        loss = 0.0\n",
    "        X_feature = x.to('cuda')\n",
    "        y = inverse_transform(y, scaler)\n",
    "        with torch.no_grad():\n",
    "            y_predict = model(X_feature).squeeze().to('cpu').detach()\n",
    "            y_predict = inverse_transform(y_predict, scaler)\n",
    "        loss = criterion(y_predict, y)\n",
    "        total_loss += loss.item()*y.shape[0]\n",
    "        count += y.shape[0]\n",
    "    return (total_loss/count)\n",
    "#eval(test_dataloader, base_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "best_val_loss = 1e8\n",
    "for epoch in range(epochs):\n",
    "    base_regression.train()\n",
    "    print (\"Training epoch: {}\".format(epoch))\n",
    "    running_loss = 0\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        X_feature = x.to('cuda')\n",
    "        y_predict = base_regression(X_feature).squeeze().to('cpu')\n",
    "        loss = criterion(y_predict, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    val_loss = eval(val_dataloader, base_regression)\n",
    "    if val_loss < best_val_loss:\n",
    "        print (\"New best validation loss: {}, saving model.\".format(val_loss))\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(base_regression.state_dict(), \"/l/ear/electrode/users/zz47/adaptation_experiments_final/base_regression_novel8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_regression.load_state_dict(torch.load(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/base_regression_novel8.pth\"))\n",
    "print (eval(test1_dataloader, base_regression))\n",
    "print (eval(test2_dataloader, base_regression))\n",
    "print (eval(test3_dataloader, base_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. l1 retrieval+adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding nearest neighbor using l1 distance\n",
    "X_val_refidx = []\n",
    "X_val_features_norm = normalize(X_val_features, axis=1, norm='l1')\n",
    "X_train_features_norm = normalize(X_train_features, axis=1, norm='l1')\n",
    "for i in range(np.shape(X_val_features_norm)[0]):\n",
    "    if (i)%100==0:\n",
    "        print (i)\n",
    "    X_val_refidx.append(l1_1NN(X_train_features_norm, X_val_features_norm[i]))\n",
    "print (\"done\")\n",
    "\n",
    "np.save(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/X_val_refidx_l1_novel8.npy\", X_val_refidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding nearest neighbor using l1 distance\n",
    "X_test1_refidx = []\n",
    "X_test1_features_norm = normalize(X_test1_features, axis=1, norm='l1')\n",
    "X_train_features_norm = normalize(X_train_features, axis=1, norm='l1')\n",
    "for i in range(np.shape(X_test1_features_norm)[0]):\n",
    "    if (i)%100==0:\n",
    "        print (i)\n",
    "    X_test1_refidx.append(l1_1NN(X_train_features_norm, X_test1_features_norm[i]))\n",
    "print (\"done\")\n",
    "\n",
    "np.save(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/X_test1_refidx_l1_novel8.npy\", X_test1_refidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding nearest neighbor using l1 distance\n",
    "X_test2_refidx = []\n",
    "X_test2_features_norm = normalize(X_test2_features, axis=1, norm='l1')\n",
    "X_train_features_norm = normalize(X_train_features, axis=1, norm='l1')\n",
    "for i in range(np.shape(X_test2_features_norm)[0]):\n",
    "    if (i)%100==0:\n",
    "        print (i)\n",
    "    X_test2_refidx.append(l1_1NN(X_train_features_norm, X_test2_features_norm[i]))\n",
    "print (\"done\")\n",
    "\n",
    "np.save(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/X_test2_refidx_l1_novel8.npy\", X_test2_refidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding nearest neighbor using l1 distance\n",
    "X_test3_refidx = []\n",
    "X_test3_features_norm = normalize(X_test3_features, axis=1, norm='l1')\n",
    "X_train_features_norm = normalize(X_train_features, axis=1, norm='l1')\n",
    "for i in range(np.shape(X_test3_features_norm)[0]):\n",
    "    if (i)%100==0:\n",
    "        print (i)\n",
    "    X_test3_refidx.append(l1_1NN(X_train_features_norm, X_test3_features_norm[i]))\n",
    "print (\"done\")\n",
    "\n",
    "np.save(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/X_test3_refidx_l1_novel8.npy\", X_test3_refidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find reference for each val sample\n",
    "X_val_ref = X_train_features[X_val_refidx]\n",
    "y_val_ref = scaled_y_train[X_val_refidx]\n",
    "X_val_diff = np.concatenate((X_val_features, X_val_ref), axis=1)\n",
    "y_val_diff = scaled_y_val-y_val_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find reference for each test sample\n",
    "X_test1_ref = X_train_features[X_test1_refidx]\n",
    "y_test1_ref = scaled_y_train[X_test1_refidx]\n",
    "X_test1_diff = np.concatenate((X_test1_features, X_test1_ref), axis=1)\n",
    "y_test1_diff = scaled_y_test1-y_test1_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find reference for each test sample\n",
    "X_test2_ref = X_train_features[X_test2_refidx]\n",
    "y_test2_ref = scaled_y_train[X_test2_refidx]\n",
    "X_test2_diff = np.concatenate((X_test2_features, X_test2_ref), axis=1)\n",
    "y_test2_diff = scaled_y_test2-y_test2_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find reference for each test sample\n",
    "X_test3_ref = X_train_features[X_test3_refidx]\n",
    "y_test3_ref = scaled_y_train[X_test3_refidx]\n",
    "X_test3_diff = np.concatenate((X_test3_features, X_test3_ref), axis=1)\n",
    "y_test3_diff = scaled_y_test3-y_test3_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is retrieval loss\n",
    "retrieval_l1_loss = np.mean(abs(inverse_transform(scaled_y_test1, scaler)-inverse_transform(y_test1_ref, scaler)))\n",
    "print (retrieval_l1_loss)\n",
    "retrieval_l1_loss = np.mean(abs(inverse_transform(scaled_y_test2, scaler)-inverse_transform(y_test2_ref, scaler)))\n",
    "print (retrieval_l1_loss)\n",
    "retrieval_l1_loss = np.mean(abs(inverse_transform(scaled_y_test3, scaler)-inverse_transform(y_test3_ref, scaler)))\n",
    "print (retrieval_l1_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new val dataset with reference\n",
    "class TrainFeatureDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.x)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        featuresN = np.shape(self.x)[0]\n",
    "        r = [*range(0,idx) ,*range(idx+1, featuresN)]\n",
    "        refidx = random.choice(r)\n",
    "        feature = self.x[idx,:].astype('float32')\n",
    "        reffeature = self.x[refidx,:].astype('float32')\n",
    "        target_class = self.y[idx].astype('float32')\n",
    "        reftarget_class = self.y[refidx].astype('float32')\n",
    "        diff_feature = torch.tensor(np.concatenate((feature, reffeature), axis=0))\n",
    "        diff_target_class = target_class-reftarget_class\n",
    "        return diff_feature, diff_target_class\n",
    "\n",
    "diff_train_dataset = TrainFeatureDataset(X_train_features, scaled_y_train)\n",
    "\n",
    "diff_train_dataloader = DataLoader(diff_train_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValFeatureDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, ref_x, ref_y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.ref_x = ref_x\n",
    "        self.ref_y = ref_y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.x)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = np.concatenate((self.x[idx,:], self.ref_x[idx,:]), axis=0).astype('float32')\n",
    "        feature = torch.tensor(feature)\n",
    "        target_class = (self.y[idx]-self.ref_y[idx]).astype('float32')\n",
    "        \n",
    "        return feature, target_class\n",
    "\n",
    "diff_val_dataset = ValFeatureDataset(X_val_features, scaled_y_val ,X_val_ref, y_val_ref)\n",
    "diff_val_dataloader = DataLoader(diff_val_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "diff_test1_dataset = ValFeatureDataset(X_test1_features, scaled_y_test1 ,X_test1_ref, y_test1_ref)\n",
    "diff_test1_dataloader = DataLoader(diff_test1_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "diff_test2_dataset = ValFeatureDataset(X_test2_features, scaled_y_test2 ,X_test2_ref, y_test2_ref)\n",
    "diff_test2_dataloader = DataLoader(diff_test2_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "diff_test3_dataset = ValFeatureDataset(X_test3_features, scaled_y_test3 ,X_test3_ref, y_test3_ref)\n",
    "diff_test3_dataloader = DataLoader(diff_test3_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptation_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(5244, 512)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(512, 16)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "adaptation_model = Adaptation_model().to('cuda')\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(adaptation_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    criterion = nn.L1Loss()\n",
    "    total_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        loss = 0.0\n",
    "        X_feature = x.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            y_predict = model(X_feature).squeeze().to('cpu').detach()\n",
    "        loss = criterion(y_predict, y) * np.sqrt(scaler.var_)\n",
    "        total_loss += loss.item()*y.shape[0]\n",
    "        count += y.shape[0]      \n",
    "    return (total_loss/(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaptation model\n",
    "epochs = 50\n",
    "best_val_loss = 1e8\n",
    "for epoch in range(epochs):\n",
    "    adaptation_model.train()\n",
    "    print (\"Training epoch: {}\".format(epoch))\n",
    "    running_loss = 0\n",
    "    for i, (x, y) in enumerate(diff_train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        X_feature = x.to('cuda')\n",
    "        y_predict = adaptation_model(X_feature).squeeze().to('cpu')\n",
    "        loss = criterion(y_predict, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    val_loss = eval(diff_val_dataloader, adaptation_model)\n",
    "    if val_loss < best_val_loss:\n",
    "        print (\"New best validation loss: {}, saving model.\".format(val_loss))\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(adaptation_model.state_dict(), \"/l/ear/electrode/users/zz47/adaptation_experiments_final/adaptation_model_novel8.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_model.load_state_dict(torch.load(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/adaptation_model_novel8.pth\"))\n",
    "print (eval(diff_test1_dataloader, adaptation_model))\n",
    "print (eval(diff_test2_dataloader, adaptation_model))\n",
    "print (eval(diff_test3_dataloader, adaptation_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Learned Distance (Siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_dist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2622, 512, bias=True)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(512, 64, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "siamese_dist = Siamese_dist().to('cuda')\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(siamese_dist.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainFeatureDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.x)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        featuresN = np.shape(self.x)[0]\n",
    "        target_class = self.y[idx].astype('float32')\n",
    "        feature = self.x[idx,:].astype('float32')\n",
    "\n",
    "        posmask = self.y == target_class\n",
    "        rpos = np.array(np.where(posmask))[0,:]\n",
    "        posidx = random.choice(rpos)\n",
    "        posfeature = self.x[posidx,:].astype('float32')\n",
    "        postarget_class = self.y[posidx].astype('float32')\n",
    "\n",
    "        negmask = (self.y <= (target_class-10)) | (self.y >= (target_class+10))\n",
    "        rneg = np.array(np.where(negmask))[0,:]\n",
    "        negidx = random.choice(rneg)\n",
    "        negfeature = self.x[negidx,:].astype('float32')\n",
    "        \n",
    "        return feature, posfeature, negfeature\n",
    "\n",
    "siamese_train_dataset = TrainFeatureDataset(X_train_features, y_train)\n",
    "\n",
    "siamese_train_dataloader = DataLoader(siamese_train_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_siamese(y_val, X_val_features):\n",
    "    Siamese_dist.eval()\n",
    "    X_train_features_64 = extract_features(X_train_features, siamese_dist)\n",
    "    X_val_features_64 = extract_features(X_val_features, siamese_dist)\n",
    "    X_val_refidx_siamese = []\n",
    "    for i in range(np.shape(X_val_features_64)[0]):\n",
    "        X_val_refidx_siamese.append(l1_1NN(X_train_features_64, X_val_features_64[i]))\n",
    "    \n",
    "    y_val_ref_siamese = y_train[X_val_refidx_siamese]\n",
    "    mae = np.mean(np.abs(y_val-y_val_ref_siamese))\n",
    "    \n",
    "    return (mae, X_val_refidx_siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1, p=1.0)\n",
    "best_mae = 1e8\n",
    "for epoch in range(epochs):\n",
    "    siamese_dist.train()\n",
    "    print (\"Training epoch: {}\".format(epoch))\n",
    "    running_loss = 0\n",
    "    for i, (x, posx, negx) in enumerate(siamese_train_dataloader):\n",
    "        #print (i)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        x = x.to('cuda')\n",
    "        posx = posx.to('cuda')\n",
    "        negx = negx.to('cuda')\n",
    "        y = siamese_dist(x).squeeze().to('cpu')\n",
    "        posy = siamese_dist(posx).squeeze().to('cpu')\n",
    "        negy = siamese_dist(negx).squeeze().to('cpu')\n",
    "        loss = triplet_loss(y, posy, negy)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    mae, _ = eval_siamese(y_val, X_val_features)\n",
    "    if mae < best_mae:\n",
    "        print (\"new best val mae: {}\".format(mae))\n",
    "        best_mae = mae\n",
    "        torch.save(siamese_dist.state_dict(), \"/l/ear/electrode/users/zz47/adaptation_experiments_final/siamese_dist_novel8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dist.load_state_dict(torch.load(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/siamese_dist_novel8.pth\"))\n",
    "_, X_val_refidx_siamese = eval_siamese(y_val, X_val_features)\n",
    "mae, X_test1_refidx_siamese = eval_siamese(y_test1, X_test1_features)\n",
    "print (mae)\n",
    "mae, X_test2_refidx_siamese = eval_siamese(y_test2, X_test2_features)\n",
    "print (mae)\n",
    "mae, X_test3_refidx_siamese = eval_siamese(y_test3, X_test3_features)\n",
    "print (mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Learned Distance (Siamese) + Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find reference for each val sample\n",
    "X_val_ref = X_train_features[X_val_refidx_siamese]\n",
    "y_val_ref = scaled_y_train[X_val_refidx_siamese]\n",
    "X_val_diff = np.concatenate((X_val_features, X_val_ref), axis=1)\n",
    "y_val_diff = scaled_y_val-y_val_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find reference for each test sample\n",
    "X_test1_ref = X_train_features[X_test1_refidx_siamese]\n",
    "y_test1_ref = scaled_y_train[X_test1_refidx_siamese]\n",
    "X_test1_diff = np.concatenate((X_test1_features, X_test1_ref), axis=1)\n",
    "y_test1_diff = scaled_y_test1-y_test1_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2_ref = X_train_features[X_test2_refidx_siamese]\n",
    "y_test2_ref = scaled_y_train[X_test2_refidx_siamese]\n",
    "X_test2_diff = np.concatenate((X_test2_features, X_test2_ref), axis=1)\n",
    "y_test2_diff = scaled_y_test2-y_test2_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test3_ref = X_train_features[X_test3_refidx_siamese]\n",
    "y_test3_ref = scaled_y_train[X_test3_refidx_siamese]\n",
    "X_test3_diff = np.concatenate((X_test3_features, X_test3_ref), axis=1)\n",
    "y_test3_diff = scaled_y_test3-y_test3_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is retrieval loss\n",
    "retrieval_l1_loss = np.mean(abs(inverse_transform(scaled_y_test1, scaler)-inverse_transform(y_test1_ref, scaler)))\n",
    "print (retrieval_l1_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new val dataset with reference\n",
    "class TrainFeatureDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.x)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        featuresN = np.shape(self.x)[0]\n",
    "        r = [*range(0,idx) ,*range(idx+1, featuresN)]\n",
    "        refidx = random.choice(r)\n",
    "        feature = self.x[idx,:].astype('float32')\n",
    "        reffeature = self.x[refidx,:].astype('float32')\n",
    "        target_class = self.y[idx].astype('float32')\n",
    "        reftarget_class = self.y[refidx].astype('float32')\n",
    "        diff_feature = torch.tensor(np.concatenate((feature, reffeature), axis=0))\n",
    "        diff_target_class = target_class-reftarget_class\n",
    "        return diff_feature, diff_target_class\n",
    "\n",
    "diff_train_dataset = TrainFeatureDataset(X_train_features, scaled_y_train)\n",
    "\n",
    "diff_train_dataloader = DataLoader(diff_train_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValFeatureDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, ref_x, ref_y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.ref_x = ref_x\n",
    "        self.ref_y = ref_y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.x)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = np.concatenate((self.x[idx,:], self.ref_x[idx,:]), axis=0).astype('float32')\n",
    "        feature = torch.tensor(feature)\n",
    "        target_class = (self.y[idx]-self.ref_y[idx]).astype('float32')\n",
    "        \n",
    "        return feature, target_class\n",
    "\n",
    "diff_val_dataset = ValFeatureDataset(X_val_features, scaled_y_val ,X_val_ref, y_val_ref)\n",
    "diff_val_dataloader = DataLoader(diff_val_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "diff_test1_dataset = ValFeatureDataset(X_test1_features, scaled_y_test1 ,X_test1_ref, y_test1_ref)\n",
    "diff_test1_dataloader = DataLoader(diff_test1_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "diff_test2_dataset = ValFeatureDataset(X_test2_features, scaled_y_test2 ,X_test2_ref, y_test2_ref)\n",
    "diff_test2_dataloader = DataLoader(diff_test2_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)\n",
    "diff_test3_dataset = ValFeatureDataset(X_test3_features, scaled_y_test3 ,X_test3_ref, y_test3_ref)\n",
    "diff_test3_dataloader = DataLoader(diff_test3_dataset, batch_size=50,\n",
    "                        shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptation_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(5244, 512)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(512, 16)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "adaptation_model = Adaptation_model().to('cuda')\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(adaptation_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    criterion = nn.L1Loss()\n",
    "    total_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        loss = 0.0\n",
    "        X_feature = x.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            y_predict = model(X_feature).squeeze().to('cpu').detach()\n",
    "        loss = criterion(y_predict, y) * np.sqrt(scaler.var_)\n",
    "        total_loss += loss.item()*y.shape[0]\n",
    "        count += y.shape[0]      \n",
    "    return (total_loss/(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_model.load_state_dict(torch.load(\"/l/ear/electrode/users/zz47/adaptation_experiments_final/adaptation_model_novel8.pth\"))\n",
    "print (eval(diff_test1_dataloader, adaptation_model))\n",
    "print (eval(diff_test2_dataloader, adaptation_model))\n",
    "print (eval(diff_test3_dataloader, adaptation_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchonly",
   "language": "python",
   "name": "pytorchonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
